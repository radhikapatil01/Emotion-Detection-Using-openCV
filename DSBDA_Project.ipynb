{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8-Y5NmopNyq"
      },
      "source": [
        "### **EMOTION DETECTION USING IMAGES AND FROM WEBCAM** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUp--rdEUQwk"
      },
      "source": [
        "Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZLMVP_IdrSN",
        "outputId": "957c53e9-0d16-4f1c-da40-caf9b73b75ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-05-06 08:39:26--  https://www.dropbox.com/s/nilt43hyl1dx82k/dataset.zip?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.85.18, 2620:100:6030:18::a27d:5012\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.85.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/nilt43hyl1dx82k/dataset.zip [following]\n",
            "--2022-05-06 08:39:26--  https://www.dropbox.com/s/raw/nilt43hyl1dx82k/dataset.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc2e2f3e8cfbfb3e9d3ef6dc322d.dl.dropboxusercontent.com/cd/0/inline/BksUC8snHV1CC2xVPcdrsuGqafZsr3T-k_StJrfVc_E3QGbeS9xSpBKIbUsHeK_KvyOiWSq8zcoZyUPaL0RKRvNd01ACGOnuOiS5KkxhFluzQ6aV_jpoHjqZx2maAZ4DdRd0oSlW2GvlB-aOfQbtKqbsw1yEuG2mPJkEKHPk6uA4OA/file# [following]\n",
            "--2022-05-06 08:39:27--  https://uc2e2f3e8cfbfb3e9d3ef6dc322d.dl.dropboxusercontent.com/cd/0/inline/BksUC8snHV1CC2xVPcdrsuGqafZsr3T-k_StJrfVc_E3QGbeS9xSpBKIbUsHeK_KvyOiWSq8zcoZyUPaL0RKRvNd01ACGOnuOiS5KkxhFluzQ6aV_jpoHjqZx2maAZ4DdRd0oSlW2GvlB-aOfQbtKqbsw1yEuG2mPJkEKHPk6uA4OA/file\n",
            "Resolving uc2e2f3e8cfbfb3e9d3ef6dc322d.dl.dropboxusercontent.com (uc2e2f3e8cfbfb3e9d3ef6dc322d.dl.dropboxusercontent.com)... 162.125.81.15, 2620:100:6030:15::a27d:500f\n",
            "Connecting to uc2e2f3e8cfbfb3e9d3ef6dc322d.dl.dropboxusercontent.com (uc2e2f3e8cfbfb3e9d3ef6dc322d.dl.dropboxusercontent.com)|162.125.81.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/BktvfgM2amCX3N6RzZ594-2zNlM8tqxexFYBdDlwKsgzqzCnuqHk5orh8cnprHLpGqcxARO-GLGDhMNmGgJVyWWKJG0ye6oMV29ag1aKKgTV3a1TuV726ulcuIeIvIKYsZsQdM37GoWh_gabNjW6OmE5WQB6WRYMP2zjhgDdXm9tzM55Gu3UQhG0L8SiBsGUDlddfTmfFnNDwPRieFUHsO2rjnp_C_SuEyBYogFoP5SL8nXsmvQ6V1Jen1ooEeX1aDM6_TjWPHkJPWQkTLVmov3jd5AWIvB9q0fMVmWATb3-11SVKHdjruaj8HfIw5l6Yi9dHaxeN8souEMtXyuBhvPLuNQWw5Okvfw2oxQuV194wPUM_ky19LUjAnyc6X142VT4QneJEEJ-ouYLTcnAimzsnSj0gtyjtcRouEAMyXATyg/file [following]\n",
            "--2022-05-06 08:39:27--  https://uc2e2f3e8cfbfb3e9d3ef6dc322d.dl.dropboxusercontent.com/cd/0/inline2/BktvfgM2amCX3N6RzZ594-2zNlM8tqxexFYBdDlwKsgzqzCnuqHk5orh8cnprHLpGqcxARO-GLGDhMNmGgJVyWWKJG0ye6oMV29ag1aKKgTV3a1TuV726ulcuIeIvIKYsZsQdM37GoWh_gabNjW6OmE5WQB6WRYMP2zjhgDdXm9tzM55Gu3UQhG0L8SiBsGUDlddfTmfFnNDwPRieFUHsO2rjnp_C_SuEyBYogFoP5SL8nXsmvQ6V1Jen1ooEeX1aDM6_TjWPHkJPWQkTLVmov3jd5AWIvB9q0fMVmWATb3-11SVKHdjruaj8HfIw5l6Yi9dHaxeN8souEMtXyuBhvPLuNQWw5Okvfw2oxQuV194wPUM_ky19LUjAnyc6X142VT4QneJEEJ-ouYLTcnAimzsnSj0gtyjtcRouEAMyXATyg/file\n",
            "Reusing existing connection to uc2e2f3e8cfbfb3e9d3ef6dc322d.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 63252113 (60M) [application/zip]\n",
            "Saving to: ‘dataset.zip?dl=0.3’\n",
            "\n",
            "dataset.zip?dl=0.3  100%[===================>]  60.32M  11.1MB/s    in 6.3s    \n",
            "\n",
            "2022-05-06 08:39:34 (9.65 MB/s) - ‘dataset.zip?dl=0.3’ saved [63252113/63252113]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.dropbox.com/s/nilt43hyl1dx82k/dataset.zip?dl=0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-P6X1iTUUve"
      },
      "source": [
        "Unziping the images from the folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNB8lSOte3WF",
        "outputId": "6db95f75-5f16-437c-a560-801b71d40917"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  dataset.zip?dl=0\n",
            "replace test/angry/PrivateTest_10131363.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!unzip dataset.zip?dl=0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rZJpx_lUY4R"
      },
      "source": [
        "Importing the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhDHNoHSgfhL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.layers import Flatten, Dense\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.image import ImageDataGenerator , img_to_array, load_img\n",
        "from keras.applications.mobilenet import MobileNet, preprocess_input \n",
        "from keras.losses import categorical_crossentropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc0mYZDAqFTU"
      },
      "source": [
        "Building our model to train the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHS5BTz7p_Co"
      },
      "outputs": [],
      "source": [
        "# Working with pre trained model \n",
        "\n",
        "base_model = MobileNet( input_shape=(224,224,3), include_top= False )\n",
        "\n",
        "for layer in base_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "\n",
        "x = Flatten()(base_model.output)\n",
        "x = Dense(units=7 , activation='softmax' )(x)\n",
        "\n",
        "# creating our model.\n",
        "model = Model(base_model.input, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZ_vDwLtqW-r"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss= categorical_crossentropy , metrics=['accuracy']  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5O27HVpelO6I"
      },
      "source": [
        "Preparing our data using data generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rX_mhspqroA"
      },
      "outputs": [],
      "source": [
        "#training the data using image data generator\n",
        "train_datagen = ImageDataGenerator(\n",
        "     zoom_range = 0.2, \n",
        "     shear_range = 0.2, \n",
        "     horizontal_flip=True, \n",
        "     rescale = 1./255\n",
        ")\n",
        "\n",
        "train_data = train_datagen.flow_from_directory(directory= \"/content/train\", \n",
        "                                               target_size=(224,224), \n",
        "                                               batch_size=32,\n",
        "                                  )\n",
        "\n",
        "\n",
        "train_data.class_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISkux9A_qv69"
      },
      "outputs": [],
      "source": [
        "#validating the model (that's why we're using image data generator)\n",
        "val_datagen = ImageDataGenerator(rescale = 1./255 )\n",
        "\n",
        "val_data = val_datagen.flow_from_directory(directory= \"/content/test\", \n",
        "                                           target_size=(224,224), \n",
        "                                           batch_size=32,\n",
        "                                  )   #batch size indicates how many time we have to run the cycle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FpArhncl30X"
      },
      "source": [
        "Visualizaing the data or plotting the images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xD08NJkqz2b"
      },
      "outputs": [],
      "source": [
        "# to visualize the images in the traing data denerator  (plotting the images)\n",
        "\n",
        "t_img , label = train_data.next()\n",
        "\n",
        "# function when called will plot the images \n",
        "def plotImages(img_arr, label):\n",
        "  \"\"\"\n",
        "  input  :- images array \n",
        "  output :- plots the images \n",
        "  \"\"\"\n",
        "  count = 0\n",
        "  for im, l in zip(img_arr,label) :\n",
        "    plt.imshow(im)\n",
        "    plt.title(im.shape)\n",
        "    plt.axis = False\n",
        "    plt.show()\n",
        "    \n",
        "    count += 1\n",
        "    if count == 10:\n",
        "      break\n",
        "\n",
        "# function call to plot the images \n",
        "plotImages(t_img, label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8CPVX0VmCv2"
      },
      "source": [
        "Having early stopping and model check point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qt_tGImJq807"
      },
      "outputs": [],
      "source": [
        "## having early stopping and model check point \n",
        "\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# early stopping\n",
        "es = EarlyStopping(monitor='val_accuracy', min_delta= 0.01 , patience= 5, verbose= 1, mode='auto')\n",
        "\n",
        "# model check point\n",
        "mc = ModelCheckpoint(filepath=\"best_model.h5\", monitor= 'val_accuracy', verbose= 1, save_best_only= True, mode = 'auto')\n",
        "\n",
        "# puting call back in a list \n",
        "call_back = [es, mc]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iA6-1ltmrBAC"
      },
      "outputs": [],
      "source": [
        "#training the model\n",
        "hist = model.fit_generator(train_data, \n",
        "                           steps_per_epoch= 10, \n",
        "                           epochs= 30, \n",
        "                           validation_data= val_data, \n",
        "                           validation_steps= 8, \n",
        "                           callbacks=[es,mc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyQ39GURnpli"
      },
      "source": [
        "Loading the trained model in best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ex4kYcfMtC_V"
      },
      "outputs": [],
      "source": [
        "# Loading the best fit model \n",
        "from keras.models import load_model\n",
        "model = load_model(\"/content/best_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bt6Q9BG3t8bI"
      },
      "outputs": [],
      "source": [
        "h =  hist.history\n",
        "h.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kZJihCtnX1w"
      },
      "source": [
        "Plotting accuracy vs validation accuracy "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0F-0rhgtt8Xv"
      },
      "outputs": [],
      "source": [
        "plt.plot(h['accuracy'])\n",
        "plt.plot(h['val_accuracy'] , c = \"red\")\n",
        "plt.title(\"acc vs v-acc\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBEZnmO4nfxG"
      },
      "source": [
        "Plotting loss vs validation loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zx80bcL0uCd5"
      },
      "outputs": [],
      "source": [
        "plt.plot(h['loss'])\n",
        "plt.plot(h['val_loss'] , c = \"red\")\n",
        "plt.title(\"loss vs v-loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xdzdj7p6uIib"
      },
      "outputs": [],
      "source": [
        "# just to map o/p values \n",
        "op = dict(zip( train_data.class_indices.values(), train_data.class_indices.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UqN0N90uL6E"
      },
      "outputs": [],
      "source": [
        "# path for the image to see if it predicts correct class\n",
        "\n",
        "path = \"/content/angry.jpg\"\n",
        "img = load_img(path, target_size=(224,224) )\n",
        "\n",
        "i = img_to_array(img)/255\n",
        "input_arr = np.array([i])\n",
        "input_arr.shape\n",
        "\n",
        "pred = np.argmax(model.predict(input_arr))\n",
        "\n",
        "print(f\" The image is of {op[pred]}\")\n",
        "\n",
        "# to display the image  \n",
        "plt.imshow(input_arr[0])\n",
        "plt.title(\"input image\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "DSBDA Project",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}